<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>  &#8211; Research interest </title>
<meta name="description" content="">
<meta name="keywords" content="tools">


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Research interest">
<meta property="og:description" content="Welcome to my site.">
<meta property="og:url" content="http://localhost:4000/projects/">
<meta property="og:site_name" content="">





<link rel="canonical" href="http://localhost:4000/projects/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title=" Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/academicons.css" />

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

</head>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4000"></a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				<li><a href="http://localhost:4000/about/" >About</a></li>
		        
				<li><a href="http://localhost:4000/publications/" >Publications</a></li>
		        
				<li><a href="http://localhost:4000/projects/" >Research Interest</a></li>
		        
				<li><a href="http://localhost:4000/pdf/CV_Francois_Rameau.pdf" >Resume</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    
	<img src="http://localhost:4000/images/profile_pic.png" class="bio-photo" alt="Francois Rameau bio photo"></a>

<h3>Francois Rameau</h3>
<p>Research Professor</p>
<a href="https://en.wikipedia.org/wiki/Daejeon" class="author-social" target="_blank"><i class="fa fa-location-arrow"></i> Daejeon, South Korea</a>

<a href="http://scholar.google.es/citations?user=Hfx_pykAAAAJ" class="author-social" target="_blank"><i class="ai ai-google-scholar-square"></i>&nbsp; Scholar</a>
<a href="https://www.researchgate.net/profile/Francois_rameau" class="author-social" target="_blank"><i class="ai ai-researchgate-square ai-3x"></i> ResearchGate</a>

<a href="http://linkedin.com/in/frameau" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/rameau-fr" class="author-social" target="_blank"><i class="fa fa-github-square"></i> Github</a>






<a href="mailto:rameau.fr@gmail.com" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> e-Mail</a>



  </div>
  <article>
    <h1>Research interest</h1>
    <div class="article-wrap">
      <p><br /></p>
<h3 id="connected-vehicles">Connected vehicles</h3>
<hr />

<div class="post-container">                
    <div class="post-thumb"><a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=RYzgyOgVzM4
" target="_blank"><img src="http://img.youtube.com/vi/RYzgyOgVzM4/0.jpg" alt="" style="float:left;width:320px; margin: 0px 20px 0px 0px;" /></a></div>
    <div class="post-content">
        <h3 class="post-title">The see-through system</h3>
        <p>In this collaboration with Bosch, we propose a real-time marker-less system to see through cars. For this purpose, two cars are equipped with cameras and an appropriate wireless communication system. The stereo vision system mounted on the front car allows to create a sparse 3D map of the environment where the rear car can be localized. Using this inter-car pose estimation, a synthetic image is generated to overcome the occlusion and to create a seamless see-through effect which preserves the structure of the scene. </p></div>
</div>

<p><br /></p>

<div class="post-container">
    <div class="post-content">
        <h3 class="post-title">Collaborative localization of a swarm of vehicle</h3>
        <p>We propose a multi-vehicle localization approach relying exclusively on cameras installed on connected cars (i.e. vehicles with Internet access). The proposed method is designed to perform in real-time while requiring a low bandwidth connection as a result of an efficient centralized/decentralized architecture. Hence, our approach is compatible with both LTE Internet connection and local WIFI networks. To reach this goal, the vehicles share small portions of their respective 3D maps to estimate their relative positions. The global consistency between multiple vehicles is enforced via a novel graph-based strategy. </p></div>
<div class="post-thumb"><img src="/images/IllustMultiCar.png" /></div>
</div>

<hr />
<p><br /></p>
<h3 id="camera-calibration">Camera calibration</h3>
<hr />

<div class="post-container">
    <div class="post-content">
<h3 class="post-title">PTZ self-calibration</h3>
<div class="post-thumb"><img src="/images/PTZHomog.png" style="float:left;width:150px; margin: 0px 20px 0px 0px;  " /></div>
        <p>We propose a multi-vehicle localization approach relying exclusively on cameras installed on connected cars (i.e. vehicles with Internet access). The proposed method is designed to perform in real-time while requiring a low bandwidth connection as a result of an efficient centralized/decentralized architecture. Hence, our approach is compatible with both LTE Internet connection and local WIFI networks. To reach this goal, the vehicles share small portions of their respective 3D maps to estimate their relative positions. The global consistency between multiple vehicles is enforced via a novel graph-based strategy. </p></div>
</div>

<p><br /></p>

<div class="post-container">
    <div class="post-content">
<h3 class="post-title">Deep-learning based camera calibration</h3>
<div class="post-thumb"><img src="/images/Undistord.png" /></div>
        <p>Calibration of wide field-of-view cameras is a fundamental step for numerous visual media production applications, such as 3D reconstruction, image undistortion, augmented reality and camera motion estimation. However, existing calibration methods require multiple images of a calibration pattern (typically a checkerboard), assume the presence of lines, require manual interaction and/or need an image sequence. In contrast, we present a novel fully automatic deep learning-based approach that overcomes all these limitations and works with a single image of general scenes. Our approach builds upon the recent developments in deep Convolutional Neural Networks (CNN): our network automatically estimates the intrinsic parameters of the camera (focal length and distortion parameter) from a single input image. In order to train the CNN, we leverage the great amount of omnidirectional images available on the Internet to automatically generate a large-scale dataset composed of millions of wide field-of-view images with ground truth intrinsic parameters. Experiments successfully demonstrated the quality of our results, both quantitatively and qualitatively. </p></div>
</div>

<div class="post-container">
    <div class="post-content">
<h3 class="post-title">Head-eye calibration</h3>
<div class="post-thumb"><img src="/images/HECalib.png" width="320" /></div> 
        <p>Through my research career, I encounter multiple scenarios involving hand-eye calibration problems. In particular in case of non-overlapping field-of-view cameras and when the camera is attached to a static referencial. For more details you can have a look into the related papers. </p></div>
</div>

<hr />
<p><br /></p>
<h3 id="detection-and-visual-tracking">Detection and Visual tracking</h3>
<hr />

<div class="post-container">
    <div class="post-content">
<h3 class="post-title">Deep-learning based tracking</h3>
<div class="post-thumb"><img src="/images/DeepTrack.png" /></div> 
        <p>We propose a novel video object segmentation algorithm based on pixel-level matching using Convolutional Neural Networks (CNN). Our network aims to distinguish the target area from the background on the basis of the pixel-level similarity between two object units. The proposed network represents a target object using features from different depth layers in order to take advantage of both the spatial details and the category-level semantic information. Furthermore, we propose a feature compression technique that drastically reduces the memory requirements while maintaining the capability of feature representation. Two-stage training (pre-training and fine-tuning) allows our network to handle any target object regardless of its category (even if the object’s type does not belong to the pre-training data) or of variations in its appearance through a video sequence. Experiments on large datasets demonstrate the effectiveness of our model - against related methods - in terms of accuracy, speed, and stability. Finally, we introduce the transferability of our network to different domains, such as the infrared data domain. </p></div>
</div>

<div class="post-container">
<h3 class="post-title">Omidirectional camera visual tracking</h3>
    <div class="post-thumb"><a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=GL33I-zMLB4
" target="_blank"><img src="http://img.youtube.com/vi/GL33I-zMLB4/0.jpg" alt="" style="float:left;width:200px; margin: 0px 20px 0px 0px;" /></a></div>
    <div class="post-content">
        <p>An effective technique for applying visual tracking algorithms to omnidirectional image sequences is presented. The method is based on a spherical image representation which allows taking into account the distortions and nonlinear resolution of omnidirectional images. Experimental results show that both deterministic and probabilistic tracking methods can effectively be adapted in order to robustly track an object with an omnidirectional camera.   </p></div>
</div>
<p><br /> <br /></p>

<div class="post-container">
    <div class="post-content">
<h3 class="post-title">3D car detection</h3>
<div class="post-thumb"><img src="/images/Seg2Reg.png" /></div> 
        <p>In fact, recent 3D object localization approaches have speed bottleneck caused by the fusion/prediction of depth information or anchor generation. To alleviate this issue, in this paper, we present a monocular 3D car localization method that takes advantage of road environments, i.e., car on the ground surface, instead of full depth estimation. Concretely, we propose two stages approach consists of segment and regression networks, called Segment2Regress. Given a single RGB image and a prior 2D object detection bounding box, 1) the segment network activates the pixels under the vehicles, which correspond to four line segments and a quadrilateral of the bottom of a vehicle 3D box projected on the image domain, and then, 2) the regression network predicts the 3D location of a car at the ground level. We stabilize the regression by introducing the coupling loss for grouping regression. Thanks to its simple structure, the proposed method is light and efficient and reach up to 80 FPS. We evaluate our approach in KITTI bird's eye view dataset and show comparable performance with fast inference.  </p></div>
</div>

<hr />
<p><br /></p>
<h3 id="3d-reconstruction">3D reconstruction</h3>
<hr />

<div class="post-container">                
    <div class="post-thumb"><a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=5g5Uu7XX_Hk
" target="_blank"><img src="http://img.youtube.com/vi/5g5Uu7XX_Hk/0.jpg" alt="" style="float:left;width:320px; margin: 0px 20px 0px 0px;" /></a></div>
    <div class="post-content">
        <h3 class="post-title">stereo-SLAM</h3>
        <p> <br /> <br /> I fully devellop my own stereo-SLAM, details coming soon. <br /> <br /> </p></div>





</div>


      <!-- <div id="index"> -->
         <!-- <h1>Recent posts</h1>-->
         <!-- -->
            <!--  <h3><a href="http://localhost:4000/jekyll/update/welcome-to-jekyll/" title="Welcome to Jekyll!">Welcome to Jekyll!</a></h3>-->
            <!--  <p>You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different wa...</p>-->
        <!--  -->
    <!--  </div> -->
    </div><!-- /.article-wrap -->
    
  </article>
</div><!-- /#index -->

<div class="footer-wrap">
  <footer>
    <span>&copy; 2020 Francois Rameau. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>





</body>
</html>
